{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "class FeatureExtractorFineTuner:\n",
        "    def __init__(self, input_shape, num_classes, learning_rates):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.learning_rates = learning_rates\n",
        "        self.feature_extractor = self.create_feature_extractor()\n",
        "        self.fine_tuning_started = False\n",
        "    \n",
        "    def create_feature_extractor(self):\n",
        "        # Load the DenseNet model pre-trained on ImageNet\n",
        "        base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=self.input_shape)\n",
        "        \n",
        "        # Freeze the convolutional layer blocks and flatten layer\n",
        "        for layer in base_model.layers:\n",
        "            if 'conv' in layer.name or 'pool' in layer.name or 'flatten' in layer.name:\n",
        "                layer.trainable = False\n",
        "        \n",
        "        # Get the output of the last convolutional layer\n",
        "        output = base_model.output\n",
        "        \n",
        "        # Add a global average pooling layer\n",
        "        output = tf.keras.layers.GlobalAveragePooling2D()(output)\n",
        "        \n",
        "        # Add a fully connected layer with softmax activation for classification\n",
        "        output = tf.keras.layers.Dense(self.num_classes, activation='softmax')(output)\n",
        "        \n",
        "        # Create the feature extraction model\n",
        "        feature_extractor = Model(inputs=base_model.input, outputs=output)\n",
        "        \n",
        "        return feature_extractor\n",
        "    \n",
        "    def fine_tune_model(self):\n",
        "        # Get the total number of layers in the model\n",
        "        num_layers = len(self.feature_extractor.layers)\n",
        "        \n",
        "        # Create a list of optimizers with different learning rates for each layer\n",
        "        optimizers = [SGD(learning_rate=lr) for lr in self.learning_rates]\n",
        "        \n",
        "        # Set the optimizers for each layer\n",
        "        for i, optimizer in enumerate(optimizers):\n",
        "            self.feature_extractor.layers[i].trainable = True\n",
        "        \n",
        "        # Compile the model with the final learning rate\n",
        "        self.feature_extractor.compile(optimizer=optimizers[-1], loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    def train(self, train_data, train_labels, epochs=10):\n",
        "        if not self.fine_tuning_started:\n",
        "            # Perform feature extraction\n",
        "            optimizer = SGD(learning_rate=self.learning_rates[0])\n",
        "            self.feature_extractor.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "            self.feature_extractor.fit(train_data, train_labels, epochs=epochs)\n",
        "            \n",
        "            # Check the accuracy threshold to start fine-tuning\n",
        "            accuracy = self.feature_extractor.history.history['accuracy'][-1]\n",
        "            if accuracy >= 0.6 and accuracy <= 0.7:\n",
        "                self.fine_tune_model()\n",
        "                self.fine_tuning_started = True\n",
        "        else:\n",
        "            # Fine-tuning\n",
        "            optimizer = SGD(learning_rate=self.learning_rates[-1])\n",
        "            self.feature_extractor.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "            self.feature_extractor.fit(train_data, train_labels, epochs=epochs)\n"
      ],
      "metadata": {
        "id": "0Svq8jhT7_mA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to the range [0, 1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert class labels to one-hot encoded vectors\n",
        "num_classes = 10\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "# Define the input shape for the feature extractor\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Define the learning rates for fine-tuning\n",
        "learning_rates = [0.001, 0.001, 0.0001]\n",
        "\n",
        "# Create an instance of FeatureExtractorFineTuner\n",
        "extractor_fine_tuner = FeatureExtractorFineTuner(input_shape, num_classes, learning_rates)\n",
        "\n",
        "# Train the model\n",
        "extractor_fine_tuner.train(x_train, y_train, epochs=20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPqZuKuO-Mih",
        "outputId": "50a1ccfb-cb72-48ff-869b-d53e4f66066d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 6s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29084464/29084464 [==============================] - 1s 0us/step\n",
            "Epoch 1/20\n",
            "1563/1563 [==============================] - 41s 16ms/step - loss: 1.9529 - accuracy: 0.3455\n",
            "Epoch 2/20\n",
            "1563/1563 [==============================] - 25s 16ms/step - loss: 1.5108 - accuracy: 0.4769\n",
            "Epoch 3/20\n",
            "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3894 - accuracy: 0.5188\n",
            "Epoch 4/20\n",
            "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3217 - accuracy: 0.5437\n",
            "Epoch 5/20\n",
            "1563/1563 [==============================] - 25s 16ms/step - loss: 1.2768 - accuracy: 0.5584\n",
            "Epoch 6/20\n",
            "1563/1563 [==============================] - 24s 16ms/step - loss: 1.2463 - accuracy: 0.5692\n",
            "Epoch 7/20\n",
            "1563/1563 [==============================] - 24s 16ms/step - loss: 1.2224 - accuracy: 0.5774\n",
            "Epoch 8/20\n",
            "1563/1563 [==============================] - 25s 16ms/step - loss: 1.2039 - accuracy: 0.5807\n",
            "Epoch 9/20\n",
            "1563/1563 [==============================] - 25s 16ms/step - loss: 1.1899 - accuracy: 0.5887\n",
            "Epoch 10/20\n",
            "1563/1563 [==============================] - 25s 16ms/step - loss: 1.1749 - accuracy: 0.5930\n",
            "Epoch 11/20\n",
            "1563/1563 [==============================] - 25s 16ms/step - loss: 1.1660 - accuracy: 0.5969\n",
            "Epoch 12/20\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.1534 - accuracy: 0.6004\n",
            "Epoch 13/20\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.1488 - accuracy: 0.6041\n",
            "Epoch 14/20\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.1466 - accuracy: 0.6036\n",
            "Epoch 15/20\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.1371 - accuracy: 0.6054\n",
            "Epoch 16/20\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.1315 - accuracy: 0.6106\n",
            "Epoch 17/20\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.1279 - accuracy: 0.6114\n",
            "Epoch 18/20\n",
            "1563/1563 [==============================] - 24s 16ms/step - loss: 1.1220 - accuracy: 0.6109\n",
            "Epoch 19/20\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.1189 - accuracy: 0.6132\n",
            "Epoch 20/20\n",
            "1563/1563 [==============================] - 23s 15ms/step - loss: 1.1180 - accuracy: 0.6130\n"
          ]
        }
      ]
    }
  ]
}